<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · SymbolicRegression.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://astroautomata.com/SymbolicRegression.jl/stable/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index_base/"><img src="../assets/logo.svg" alt="SymbolicRegression.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../index_base/">SymbolicRegression.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index_base/">Contents</a></li><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#equation_search"><span>equation_search</span></a></li><li><a class="tocitem" href="#Options"><span>Options</span></a></li><li><a class="tocitem" href="#Printing"><span>Printing</span></a></li><li><a class="tocitem" href="#Evaluation"><span>Evaluation</span></a></li><li><a class="tocitem" href="#Derivatives"><span>Derivatives</span></a></li><li><a class="tocitem" href="#SymbolicUtils.jl-interface"><span>SymbolicUtils.jl interface</span></a></li><li><a class="tocitem" href="#Pareto-frontier"><span>Pareto frontier</span></a></li></ul></li><li><a class="tocitem" href="../losses/">Losses</a></li><li><a class="tocitem" href="../types/">Types</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h1><h2 id="equation_search"><a class="docs-heading-anchor" href="#equation_search">equation_search</a><a id="equation_search-1"></a><a class="docs-heading-anchor-permalink" href="#equation_search" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.equation_search-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}}} where T&lt;:Number" href="#SymbolicRegression.equation_search-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}}} where T&lt;:Number"><code>SymbolicRegression.equation_search</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">equation_search(X, y[; kws...])</code></pre><p>Perform a distributed equation search for functions <code>f_i</code> which describe the mapping <code>f_i(X[:, j]) ≈ y[i, j]</code>. Options are configured using SymbolicRegression.Options(...), which should be passed as a keyword argument to options. One can turn off parallelism with <code>numprocs=0</code>, which is useful for debugging and profiling.</p><p><strong>Arguments</strong></p><ul><li><code>X::AbstractMatrix{T}</code>:  The input dataset to predict <code>y</code> from.   The first dimension is features, the second dimension is rows.</li><li><code>y::Union{AbstractMatrix{T}, AbstractVector{T}}</code>: The values to predict. The first dimension   is the output feature to predict with each equation, and the   second dimension is rows.</li><li><code>niterations::Int=10</code>: The number of iterations to perform the search.   More iterations will improve the results.</li><li><code>weights::Union{AbstractMatrix{T}, AbstractVector{T}, Nothing}=nothing</code>: Optionally   weight the loss for each <code>y</code> by this value (same shape as <code>y</code>).</li><li><code>variable_names::Union{Vector{String}, Nothing}=nothing</code>: The names   of each feature in <code>X</code>, which will be used during printing of equations.</li><li><code>options::Options=Options()</code>: The options for the search, such as   which operators to use, evolution hyperparameters, etc.</li><li><code>parallelism=:multithreading</code>: What parallelism mode to use.   The options are <code>:multithreading</code>, <code>:multiprocessing</code>, and <code>:serial</code>.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using <code>:multithreading</code>   mode, the number of threads available to julia are used. If using   <code>:multiprocessing</code>, <code>numprocs</code> processes will be created dynamically if   <code>procs</code> is unset. If you have already allocated processes, pass them   to the <code>procs</code> argument and they will be used.   You may also pass a string instead of a symbol, like <code>&quot;multithreading&quot;</code>.</li><li><code>numprocs::Union{Int, Nothing}=nothing</code>:  The number of processes to use,   if you want <code>equation_search</code> to set this up automatically. By default   this will be <code>4</code>, but can be any number (you should pick a number &lt;=   the number of cores available).</li><li><code>procs::Union{Vector{Int}, Nothing}=nothing</code>: If you have set up   a distributed run manually with <code>procs = addprocs()</code> and <code>@everywhere</code>,   pass the <code>procs</code> to this keyword argument.</li><li><code>addprocs_function::Union{Function, Nothing}=nothing</code>: If using multiprocessing   (<code>parallelism=:multithreading</code>), and are not passing <code>procs</code> manually,   then they will be allocated dynamically using <code>addprocs</code>. However,   you may also pass a custom function to use instead of <code>addprocs</code>.   This function should take a single positional argument,   which is the number of processes to use, as well as the <code>lazy</code> keyword argument.   For example, if set up on a slurm cluster, you could pass   <code>addprocs_function = addprocs_slurm</code>, which will set up slurm processes.</li><li><code>runtests::Bool=true</code>: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.</li><li><code>saved_state::Union{StateType, Nothing}=nothing</code>: If you have already   run <code>equation_search</code> and want to resume it, pass the state here.   To get this to work, you need to have set return<em>state=true,   which will cause `equation</em>search` to return the state. Note that   you cannot change the operators or dataset, but most other options   should be changeable.</li><li><code>return_state::Union{Bool, Nothing}=nothing</code>: Whether to return the   state of the search for warm starts. By default this is false.</li><li><code>loss_type::Type=Nothing</code>: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data <code>::Complex{L}</code>, then the loss   type will automatically be set to <code>L</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>hallOfFame::HallOfFame</code>: The best equations seen during the search.   hallOfFame.members gives an array of <code>PopMember</code> objects, which   have their tree (equation) stored in <code>.tree</code>. Their score (loss)   is given in <code>.score</code>. The array of <code>PopMember</code> objects   is enumerated by size from <code>1</code> to <code>options.maxsize</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/src/SymbolicRegression.jl#L234-L304">source</a></section></article><h2 id="Options"><a class="docs-heading-anchor" href="#Options">Options</a><a id="Options-1"></a><a class="docs-heading-anchor-permalink" href="#Options" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>Options(;)</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.CoreModule.OptionsStructModule.MutationWeights-Tuple{}" href="#SymbolicRegression.CoreModule.OptionsStructModule.MutationWeights-Tuple{}"><code>SymbolicRegression.CoreModule.OptionsStructModule.MutationWeights</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">MutationWeights(;kws...)</code></pre><p>This defines how often different mutations occur. These weightings will be normalized to sum to 1.0 after initialization.</p><p><strong>Arguments</strong></p><ul><li><code>mutate_constant::Float64</code>: How often to mutate a constant.</li><li><code>mutate_operator::Float64</code>: How often to mutate an operator.</li><li><code>add_node::Float64</code>: How often to append a node to the tree.</li><li><code>insert_node::Float64</code>: How often to insert a node into the tree.</li><li><code>delete_node::Float64</code>: How often to delete a node from the tree.</li><li><code>simplify::Float64</code>: How often to simplify the tree.</li><li><code>randomize::Float64</code>: How often to create a random tree.</li><li><code>do_nothing::Float64</code>: How often to do nothing.</li><li><code>optimize::Float64</code>: How often to optimize the constants in the tree, as a mutation. Note that this is different from <code>optimizer_probability</code>, which is performed at the end of an iteration for all individuals.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/src/OptionsStruct.jl#L22-L39">source</a></section></article><h2 id="Printing"><a class="docs-heading-anchor" href="#Printing">Printing</a><a id="Printing-1"></a><a class="docs-heading-anchor-permalink" href="#Printing" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DynamicExpressions.EquationModule.string_tree-Tuple{Node, Options}" href="#DynamicExpressions.EquationModule.string_tree-Tuple{Node, Options}"><code>DynamicExpressions.EquationModule.string_tree</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">string_tree(tree::Node, options::Options; kws...)</code></pre><p>Convert an equation to a string.</p><p><strong>Arguments</strong></p><ul><li><code>tree::Node</code>: The equation to convert to a string.</li><li><code>options::Options</code>: The options holding the definition of operators.</li><li><code>variable_names::Union{Array{String, 1}, Nothing}=nothing</code>: what variables   to print for each feature.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/src/InterfaceDynamicExpressions.jl#L120-L131">source</a></section></article><h2 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DynamicExpressions.EvaluateEquationModule.eval_tree_array-Tuple{Node, AbstractMatrix, Options}" href="#DynamicExpressions.EvaluateEquationModule.eval_tree_array-Tuple{Node, AbstractMatrix, Options}"><code>DynamicExpressions.EvaluateEquationModule.eval_tree_array</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eval_tree_array(tree::Node, X::AbstractArray, options::Options; kws...)</code></pre><p>Evaluate a binary tree (equation) over a given input data matrix. The operators contain all of the operators used. This function fuses doublets and triplets of operations for lower memory usage.</p><p>This function can be represented by the following pseudocode:</p><pre><code class="nohighlight hljs">function eval(current_node)
    if current_node is leaf
        return current_node.value
    elif current_node is degree 1
        return current_node.operator(eval(current_node.left_child))
    else
        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))</code></pre><p>The bulk of the code is for optimizations and pre-emptive NaN/Inf checks, which speed up evaluation significantly.</p><p><strong>Arguments</strong></p><ul><li><code>tree::Node</code>: The root node of the tree to evaluate.</li><li><code>X::AbstractArray</code>: The input data to evaluate the tree on.</li><li><code>options::Options</code>: Options used to define the operators used in the tree.</li></ul><p><strong>Returns</strong></p><ul><li><code>(output, complete)::Tuple{AbstractVector, Bool}</code>: the result,   which is a 1D array, as well as if the evaluation completed   successfully (true/false). A <code>false</code> complete means an infinity   or nan was encountered, and a large loss should be assigned   to the equation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/src/InterfaceDynamicExpressions.jl#L17-L49">source</a></section></article><h2 id="Derivatives"><a class="docs-heading-anchor" href="#Derivatives">Derivatives</a><a id="Derivatives-1"></a><a class="docs-heading-anchor-permalink" href="#Derivatives" title="Permalink"></a></h2><p><code>SymbolicRegression.jl</code> can automatically and efficiently compute derivatives of expressions with respect to variables or constants. This is done using either <code>eval_diff_tree_array</code>, to compute derivative with respect to a single variable, or with <code>eval_grad_tree_array</code>, to compute the gradient with respect all variables (or, all constants). Both use forward-mode automatic, but use <code>Zygote.jl</code> to compute derivatives of each operator, so this is very efficient.</p><article class="docstring"><header><a class="docstring-binding" id="DynamicExpressions.EvaluateEquationDerivativeModule.eval_diff_tree_array-Tuple{Node, AbstractMatrix, Options, Int64}" href="#DynamicExpressions.EvaluateEquationDerivativeModule.eval_diff_tree_array-Tuple{Node, AbstractMatrix, Options, Int64}"><code>DynamicExpressions.EvaluateEquationDerivativeModule.eval_diff_tree_array</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eval_diff_tree_array(tree::Node, X::AbstractArray, options::Options, direction::Int)</code></pre><p>Compute the forward derivative of an expression, using a similar structure and optimization to eval<em>tree</em>array. <code>direction</code> is the index of a particular variable in the expression. e.g., <code>direction=1</code> would indicate derivative with respect to <code>x1</code>.</p><p><strong>Arguments</strong></p><ul><li><code>tree::Node</code>: The expression tree to evaluate.</li><li><code>X::AbstractArray</code>: The data matrix, with each column being a data point.</li><li><code>options::Options</code>: The options containing the operators used to create the <code>tree</code>.   <code>enable_autodiff</code> must be set to <code>true</code> when creating the options.   This is needed to create the derivative operations.</li><li><code>direction::Int</code>: The index of the variable to take the derivative with respect to.</li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, derivative, complete)::Tuple{AbstractVector, AbstractVector, Bool}</code>: the normal evaluation,   the derivative, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/src/InterfaceDynamicExpressions.jl#L54-L75">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DynamicExpressions.EvaluateEquationDerivativeModule.eval_grad_tree_array-Tuple{Node, AbstractMatrix, Options}" href="#DynamicExpressions.EvaluateEquationDerivativeModule.eval_grad_tree_array-Tuple{Node, AbstractMatrix, Options}"><code>DynamicExpressions.EvaluateEquationDerivativeModule.eval_grad_tree_array</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eval_grad_tree_array(tree::Node, X::AbstractArray, options::Options; variable::Bool=false)</code></pre><p>Compute the forward-mode derivative of an expression, using a similar structure and optimization to eval<em>tree</em>array. <code>variable</code> specifies whether we should take derivatives with respect to features (i.e., <code>X</code>), or with respect to every constant in the expression.</p><p><strong>Arguments</strong></p><ul><li><code>tree::Node</code>: The expression tree to evaluate.</li><li><code>X::AbstractArray</code>: The data matrix, with each column being a data point.</li><li><code>options::Options</code>: The options containing the operators used to create the <code>tree</code>.   <code>enable_autodiff</code> must be set to <code>true</code> when creating the options.   This is needed to create the derivative operations.</li><li><code>variable::Bool</code>: Whether to take derivatives with respect to features (i.e., <code>X</code> - with <code>variable=true</code>),   or with respect to every constant in the expression (<code>variable=false</code>).</li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, gradient, complete)::Tuple{AbstractVector, AbstractArray, Bool}</code>: the normal evaluation,   the gradient, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/src/InterfaceDynamicExpressions.jl#L82-L104">source</a></section></article><h2 id="SymbolicUtils.jl-interface"><a class="docs-heading-anchor" href="#SymbolicUtils.jl-interface">SymbolicUtils.jl interface</a><a id="SymbolicUtils.jl-interface-1"></a><a class="docs-heading-anchor-permalink" href="#SymbolicUtils.jl-interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic-Tuple{Node, Options}" href="#DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic-Tuple{Node, Options}"><code>DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">node_to_symbolic(tree::Node, options::Options; kws...)</code></pre><p>Convert an expression to SymbolicUtils.jl form. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/ext/SymbolicRegressionSymbolicUtilsExt.jl#L14-L18">source</a></section></article><p>Note that use of this function requires <code>SymbolicUtils.jl</code> to be installed and loaded.</p><h2 id="Pareto-frontier"><a class="docs-heading-anchor" href="#Pareto-frontier">Pareto frontier</a><a id="Pareto-frontier-1"></a><a class="docs-heading-anchor-permalink" href="#Pareto-frontier" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.HallOfFameModule.calculate_pareto_frontier-Union{Tuple{HallOfFame{T, L}}, Tuple{L}, Tuple{T}} where {T&lt;:Number, L&lt;:Real}" href="#SymbolicRegression.HallOfFameModule.calculate_pareto_frontier-Union{Tuple{HallOfFame{T, L}}, Tuple{L}, Tuple{T}} where {T&lt;:Number, L&lt;:Real}"><code>SymbolicRegression.HallOfFameModule.calculate_pareto_frontier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">calculate_pareto_frontier(hallOfFame::HallOfFame{T,L}) where {T&lt;:DATA_TYPE,L&lt;:LOSS_TYPE}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/87eb32556ddd06e2a20fbd96958b7091a09403b8/src/HallOfFame.jl#L71-L73">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../losses/">Losses »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Wednesday 5 July 2023 13:17">Wednesday 5 July 2023</span>. Using Julia version 1.9.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
