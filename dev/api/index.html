<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · SymbolicRegression.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://astroautomata.com/SymbolicRegression.jl/stable/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">SymbolicRegression.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">SymbolicRegression.jl</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#EquationSearch"><span>EquationSearch</span></a></li><li><a class="tocitem" href="#Options"><span>Options</span></a></li><li><a class="tocitem" href="#Printing"><span>Printing</span></a></li><li><a class="tocitem" href="#Evaluation"><span>Evaluation</span></a></li><li><a class="tocitem" href="#Derivatives"><span>Derivatives</span></a></li><li><a class="tocitem" href="#SymbolicUtils.jl-interface"><span>SymbolicUtils.jl interface</span></a></li><li><a class="tocitem" href="#Pareto-frontier"><span>Pareto frontier</span></a></li></ul></li><li><a class="tocitem" href="../losses/">Losses</a></li><li><a class="tocitem" href="../types/">Types</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h1><h2 id="EquationSearch"><a class="docs-heading-anchor" href="#EquationSearch">EquationSearch</a><a id="EquationSearch-1"></a><a class="docs-heading-anchor-permalink" href="#EquationSearch" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.EquationSearch-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}}} where T&lt;:Real" href="#SymbolicRegression.EquationSearch-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}}} where T&lt;:Real"><code>SymbolicRegression.EquationSearch</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">EquationSearch(X, y[; kws...])</code></pre><p>Perform a distributed equation search for functions <code>f_i</code> which describe the mapping <code>f_i(X[:, j]) ≈ y[i, j]</code>. Options are configured using SymbolicRegression.Options(...), which should be passed as a keyword argument to options. One can turn off parallelism with <code>numprocs=0</code>, which is useful for debugging and profiling.</p><p><strong>Arguments</strong></p><ul><li><code>X::AbstractMatrix{T}</code>:  The input dataset to predict <code>y</code> from.   The first dimension is features, the second dimension is rows.</li><li><code>y::Union{AbstractMatrix{T}, AbstractVector{T}}</code>: The values to predict. The first dimension   is the output feature to predict with each equation, and the   second dimension is rows.</li><li><code>niterations::Int=10</code>: The number of iterations to perform the search.   More iterations will improve the results.</li><li><code>weights::Union{AbstractMatrix{T}, AbstractVector{T}, Nothing}=nothing</code>: Optionally   weight the loss for each <code>y</code> by this value (same shape as <code>y</code>).</li><li><code>varMap::Union{Array{String, 1}, Nothing}=nothing</code>: The names   of each feature in <code>X</code>, which will be used during printing of equations.</li><li><code>options::Options=Options()</code>: The options for the search, such as   which operators to use, evolution hyperparameters, etc.</li><li><code>numprocs::Union{Int, Nothing}=nothing</code>:  The number of processes to use,   if you want <code>EquationSearch</code> to set this up automatically. By default   this will be <code>4</code>, but can be any number (you should pick a number &lt;=   the number of cores available).</li><li><code>procs::Union{Array{Int, 1}, Nothing}=nothing</code>: If you have set up   a distributed run manually with <code>procs = addprocs()</code> and <code>@everywhere</code>,   pass the <code>procs</code> to this keyword argument.</li><li><code>multithreading::Bool=false</code>: Whether to use multithreading. Otherwise,   will use multiprocessing. Multithreading uses less memory, but multiprocessing   can handle multi-node compute.</li><li><code>runtests::Bool=true</code>: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.</li><li><code>saved_state::Union{StateType, Nothing}=nothing</code>: If you have already   run <code>EquationSearch</code> and want to resume it, pass the state here.   To get this to work, you need to have stateReturn=true in the options,   which will cause <code>EquationSearch</code> to return the state. Note that   you cannot change the operators or dataset, but most other options   should be changeable.</li><li><code>addprocs_function::Union{Function, Nothing}=nothing</code>: If using distributed   mode (<code>multithreading=false</code>), you may pass a custom function to use   instead of <code>addprocs</code>. This function should take a single positional argument,   which is the number of processes to use, as well as the <code>lazy</code> keyword argument.   For example, if set up on a slurm cluster, you could pass   <code>addprocs_function = addprocs_slurm</code>, which will set up slurm processes.</li></ul><p><strong>Returns</strong></p><ul><li><code>hallOfFame::HallOfFame</code>: The best equations seen during the search.   hallOfFame.members gives an array of <code>PopMember</code> objects, which   have their tree (equation) stored in <code>.tree</code>. Their score (loss)   is given in <code>.score</code>. The array of <code>PopMember</code> objects   is enumerated by size from <code>1</code> to <code>options.maxsize</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/SymbolicRegression.jl#L204-L260">source</a></section></article><h2 id="Options"><a class="docs-heading-anchor" href="#Options">Options</a><a id="Options-1"></a><a class="docs-heading-anchor-permalink" href="#Options" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.CoreModule.OptionsStructModule.Options-Union{Tuple{}, Tuple{nbin}, Tuple{nuna}} where {nuna, nbin}" href="#SymbolicRegression.CoreModule.OptionsStructModule.Options-Union{Tuple{}, Tuple{nbin}, Tuple{nuna}} where {nuna, nbin}"><code>SymbolicRegression.CoreModule.OptionsStructModule.Options</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Options(;kws...)</code></pre><p>Construct options for <code>EquationSearch</code> and other functions. The current arguments have been tuned using the median values from https://github.com/MilesCranmer/PySR/discussions/115.</p><p><strong>Arguments</strong></p><ul><li><code>binary_operators</code>: Tuple of binary operators to use. Each operator should   be defined for two input scalars, and one output scalar. All operators   need to be defined over the entire real line (excluding infinity - these   are stopped before they are input), or return <code>NaN</code> where not defined.   Thus, <code>log</code> should be replaced with <code>safe_log</code>, etc.   For speed, define it so it takes two reals   of the same type as input, and outputs the same type. For the SymbolicUtils   simplification backend, you will need to define a generic method of the   operator so it takes arbitrary types.</li><li><code>unary_operators</code>: Same, but for   unary operators (one input scalar, gives an output scalar).</li><li><code>constraints</code>: Array of pairs specifying size constraints   for each operator. The constraints for a binary operator should be a 2-tuple   (e.g., <code>(-1, -1)</code>) and the constraints for a unary operator should be an <code>Int</code>.   A size constraint is a limit to the size of the subtree   in each argument of an operator. e.g., <code>[(^)=&gt;(-1, 3)]</code> means that the   <code>^</code> operator can have arbitrary size (<code>-1</code>) in its left argument,   but a maximum size of <code>3</code> in its right argument. Default is   no constraints.</li><li><code>batching</code>: Whether to evolve based on small mini-batches of data,   rather than the entire dataset.</li><li><code>batchSize</code>: What batch size to use if using batching.</li><li><code>loss</code>: What loss function to use. Can be one of   the following losses, or any other loss of type   <code>SupervisedLoss</code>. You can also pass a function that takes   a scalar target (left argument), and scalar predicted (right   argument), and returns a scalar. This will be averaged   over the predicted data. If weights are supplied, your   function should take a third argument for the weight scalar.   Included losses:       Regression:           - <code>LPDistLoss{P}()</code>,           - <code>L1DistLoss()</code>,           - <code>L2DistLoss()</code> (mean square),           - <code>LogitDistLoss()</code>,           - <code>HuberLoss(d)</code>,           - <code>L1EpsilonInsLoss(ϵ)</code>,           - <code>L2EpsilonInsLoss(ϵ)</code>,           - <code>PeriodicLoss(c)</code>,           - <code>QuantileLoss(τ)</code>,       Classification:           - <code>ZeroOneLoss()</code>,           - <code>PerceptronLoss()</code>,           - <code>L1HingeLoss()</code>,           - <code>SmoothedL1HingeLoss(γ)</code>,           - <code>ModifiedHuberLoss()</code>,           - <code>L2MarginLoss()</code>,           - <code>ExpLoss()</code>,           - <code>SigmoidLoss()</code>,           - <code>DWDMarginLoss(q)</code>.</li><li><code>npopulations</code>: How many populations of equations to use. By default   this is set equal to the number of cores</li><li><code>npop</code>: How many equations in each population.</li><li><code>ncyclesperiteration</code>: How many generations to consider per iteration.</li><li><code>ns</code>: Number of equations in each subsample during regularized evolution.</li><li><code>topn</code>: Number of equations to return to the host process, and to   consider for the hall of fame.</li><li><code>complexity_of_operators</code>: What complexity should be assigned to each operator,   and the occurrence of a constant or variable. By default, this is 1   for all operators. Can be a real number as well, in which case   the complexity of an expression will be rounded to the nearest integer.   Input this in the form of, e.g., [(^) =&gt; 3, sin =&gt; 2].</li><li><code>complexity_of_constants</code>: What complexity should be assigned to use of a constant.   By default, this is 1.</li><li><code>complexity_of_variables</code>: What complexity should be assigned to each variable.   By default, this is 1.</li><li><code>alpha</code>: The probability of accepting an equation mutation   during regularized evolution is given by exp(-delta_loss/(alpha * T)),   where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.</li><li><code>maxsize</code>: Maximum size of equations during the search.</li><li><code>maxdepth</code>: Maximum depth of equations during the search, by default   this is set equal to the maxsize.</li><li><code>parsimony</code>: A multiplicative factor for how much complexity is   punished.</li><li><code>useFrequency</code>: Whether to use a parsimony that adapts to the   relative proportion of equations at each complexity; this will   ensure that there are a balanced number of equations considered   for every complexity.</li><li><code>useFrequencyInTournament</code>: Whether to use the adaptive parsimony described   above inside the score, rather than just at the mutation accept/reject stage.</li><li><code>fast_cycle</code>: Whether to thread over subsamples of equations during   regularized evolution. Slightly improves performance, but is a different   algorithm.</li><li><code>migration</code>: Whether to migrate equations between processes.</li><li><code>hofMigration</code>: Whether to migrate equations from the hall of fame   to processes.</li><li><code>fractionReplaced</code>: What fraction of each population to replace with   migrated equations at the end of each cycle.</li><li><code>fractionReplacedHof</code>: What fraction to replace with hall of fame   equations at the end of each cycle.</li><li><code>shouldOptimizeConstants</code>: Whether to use an optimization algorithm   to periodically optimize constants in equations.</li><li><code>optimizer_nrestarts</code>: How many different random starting positions to consider   for optimization of constants.</li><li><code>optimizer_algorithm</code>: Select algorithm to use for optimizing constants. Default   is &quot;BFGS&quot;, but &quot;NelderMead&quot; is also supported.</li><li><code>optimizer_options</code>: General options for the constant optimization. For details   we refer to the documentation on <code>Optim.Options</code> from the <code>Optim.jl</code> package.   Options can be provided here as <code>NamedTuple</code>, e.g. <code>(iterations=16,)</code>, as a   <code>Dict</code>, e.g. Dict(:x_tol =&gt; 1.0e-32,), or as an <code>Optim.Options</code> instance.</li><li><code>hofFile</code>: What file to store equations to, as a backup.</li><li><code>perturbationFactor</code>: When mutating a constant, either   multiply or divide by (1+perturbationFactor)^(rand()+1).</li><li><code>probNegate</code>: Probability of negating a constant in the equation   when mutating it.</li><li><code>mutationWeights</code>: Relative probabilities of the mutations, in the order: MutateConstant, MutateOperator, AddNode, InsertNode, DeleteNode, Simplify, Randomize, DoNothing.</li><li><code>annealing</code>: Whether to use simulated annealing.</li><li><code>warmupMaxsize</code>: Whether to slowly increase the max size from 5 up to   <code>maxsize</code>. If nonzero, specifies how many cycles (populations*iterations)   before increasing by 1.</li><li><code>verbosity</code>: Whether to print debugging statements or   not.</li><li><code>bin_constraints</code>: See <code>constraints</code>. This is the same, but specified for binary   operators only (for example, if you have an operator that is both a binary   and unary operator).</li><li><code>una_constraints</code>: Likewise, for unary operators.</li><li><code>seed</code>: What random seed to use. <code>nothing</code> uses no seed.</li><li><code>progress</code>: Whether to use a progress bar output (<code>verbosity</code> will   have no effect).</li><li><code>probPickFirst</code>: Expressions in subsample are chosen based on, for   p=probPickFirst: p, p<em>(1-p), p</em>(1-p)^2, and so on.</li><li><code>earlyStopCondition</code>: Float - whether to stop early if the mean loss gets below this value.   Function - a function taking (loss, complexity) as arguments and returning true or false.</li><li><code>timeout_in_seconds</code>: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).</li><li><code>max_evals</code>: Int (or Nothing) - the maximum number of evaluations of expressions to perform.</li><li><code>skip_mutation_failures</code>: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated   expression with the original expression and proceed normally.</li><li><code>enable_autodiff</code>: Whether to enable automatic differentiation functionality. This is turned off by default.   If turned on, this will be turned off if one of the operators does not have well-defined gradients.</li><li><code>nested_constraints</code>: Specifies how many times a combination of operators can be nested. For example,   <code>[sin =&gt; [cos =&gt; 0], cos =&gt; [cos =&gt; 2]]</code> specifies that <code>cos</code> may never appear within a <code>sin</code>,   but <code>sin</code> can be nested with itself an unlimited number of times. The second term specifies that <code>cos</code>   can be nested up to 2 times within a <code>cos</code>, so that <code>cos(cos(cos(x)))</code> is allowed (as well as any combination   of <code>+</code> or <code>-</code> within it), but <code>cos(cos(cos(cos(x))))</code> is not allowed. When an operator is not specified,   it is assumed that it can be nested an unlimited number of times. This requires that there is no operator   which is used both in the unary operators and the binary operators (e.g., <code>-</code> could be both subtract, and negation).   For binary operators, both arguments are treated the same way, and the max of each argument is constrained.</li><li><code>deterministic</code>: Use a global counter for the birth time, rather than calls to <code>time()</code>. This gives   perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used   in serial mode.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/Options.jl#L122-L270">source</a></section></article><h2 id="Printing"><a class="docs-heading-anchor" href="#Printing">Printing</a><a id="Printing-1"></a><a class="docs-heading-anchor-permalink" href="#Printing" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.CoreModule.EquationModule.string_tree-Tuple{Node, Options}" href="#SymbolicRegression.CoreModule.EquationModule.string_tree-Tuple{Node, Options}"><code>SymbolicRegression.CoreModule.EquationModule.string_tree</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">string_tree(tree::Node, options::Options; kws...)</code></pre><p>Convert an equation to a string.</p><p><strong>Arguments</strong></p><ul><li><code>varMap::Union{Array{String, 1}, Nothing}=nothing</code>: what variables   to print for each feature.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/Equation.jl#L299-L308">source</a></section></article><h2 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.EvaluateEquationModule.eval_tree_array-Union{Tuple{T}, Tuple{Node{T}, AbstractMatrix{T}, Options}} where T&lt;:Real" href="#SymbolicRegression.EvaluateEquationModule.eval_tree_array-Union{Tuple{T}, Tuple{Node{T}, AbstractMatrix{T}, Options}} where T&lt;:Real"><code>SymbolicRegression.EvaluateEquationModule.eval_tree_array</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eval_tree_array(tree::Node, cX::AbstractMatrix{T}, options::Options)</code></pre><p>Evaluate a binary tree (equation) over a given input data matrix. The options contain all of the operators used. This function fuses doublets and triplets of operations for lower memory usage.</p><p>This function can be represented by the following pseudocode:</p><pre><code class="nohighlight hljs">function eval(current_node)
    if current_node is leaf
        return current_node.value
    elif current_node is degree 1
        return current_node.operator(eval(current_node.left_child))
    else
        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))</code></pre><p>The bulk of the code is for optimizations and pre-emptive NaN/Inf checks, which speed up evaluation significantly.</p><p><strong>Returns</strong></p><ul><li><code>(output, complete)::Tuple{AbstractVector{T}, Bool}</code>: the result,   which is a 1D array, as well as if the evaluation completed   successfully (true/false). A <code>false</code> complete means an infinity   or nan was encountered, and a large loss should be assigned   to the equation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/EvaluateEquation.jl#L28-L56">source</a></section></article><h2 id="Derivatives"><a class="docs-heading-anchor" href="#Derivatives">Derivatives</a><a id="Derivatives-1"></a><a class="docs-heading-anchor-permalink" href="#Derivatives" title="Permalink"></a></h2><p><code>SymbolicRegression.jl</code> can automatically and efficiently compute derivatives of expressions with respect to variables or constants. This is done using either <code>eval_diff_tree_array</code>, to compute derivative with respect to a single variable, or with <code>eval_grad_tree_array</code>, to compute the gradient with respect all variables (or, all constants). Both use forward-mode automatic, but use <code>Zygote.jl</code> to compute derivatives of each operator, so this is very efficient.</p><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.EvaluateEquationDerivativeModule.eval_diff_tree_array-Union{Tuple{T}, Tuple{Node{T}, AbstractMatrix{T}, Options, Int64}} where T&lt;:Real" href="#SymbolicRegression.EvaluateEquationDerivativeModule.eval_diff_tree_array-Union{Tuple{T}, Tuple{Node{T}, AbstractMatrix{T}, Options, Int64}} where T&lt;:Real"><code>SymbolicRegression.EvaluateEquationDerivativeModule.eval_diff_tree_array</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eval_diff_tree_array(tree::Node{T}, cX::AbstractMatrix{T}, options::Options, direction::Int)</code></pre><p>Compute the forward derivative of an expression, using a similar structure and optimization to eval<em>tree</em>array. <code>direction</code> is the index of a particular variable in the expression. e.g., <code>direction=1</code> would indicate derivative with respect to <code>x1</code>.</p><p><strong>Arguments</strong></p><ul><li><code>tree::Node</code>: The expression tree to evaluate.</li><li><code>cX::AbstractMatrix{T}</code>: The data matrix, with each column being a data point.</li><li><code>options::Options</code>: The options used to create the <code>tree</code>. Note that <code>options.enable_autodiff</code>   must be <code>true</code>. This is needed to create the derivative operations.</li><li><code>direction::Int</code>: The index of the variable to take the derivative with respect to.</li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, derivative, complete)::Tuple{AbstractVector{T}, AbstractVector{T}, Bool}</code>: the normal evaluation,   the derivative, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/EvaluateEquationDerivative.jl#L9-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.EvaluateEquationDerivativeModule.eval_grad_tree_array-Union{Tuple{T}, Tuple{Node{T}, AbstractMatrix{T}, Options}} where T&lt;:Real" href="#SymbolicRegression.EvaluateEquationDerivativeModule.eval_grad_tree_array-Union{Tuple{T}, Tuple{Node{T}, AbstractMatrix{T}, Options}} where T&lt;:Real"><code>SymbolicRegression.EvaluateEquationDerivativeModule.eval_grad_tree_array</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eval_grad_tree_array(tree::Node{T}, cX::AbstractMatrix{T}, options::Options; variable::Bool=false)</code></pre><p>Compute the forward-mode derivative of an expression, using a similar structure and optimization to eval<em>tree</em>array. <code>variable</code> specifies whether we should take derivatives with respect to features (i.e., cX), or with respect to every constant in the expression.</p><p><strong>Arguments</strong></p><ul><li><code>tree::Node{T}</code>: The expression tree to evaluate.</li><li><code>cX::AbstractMatrix{T}</code>: The data matrix, with each column being a data point.</li><li><code>options::Options</code>: The options used to create the <code>tree</code>. Note that <code>options.enable_autodiff</code>   must be <code>true</code>. This is needed to create the derivative operations.</li><li><code>variable::Bool</code>: Whether to take derivatives with respect to features (i.e., <code>cX</code> - with <code>variable=true</code>),   or with respect to every constant in the expression (<code>variable=false</code>).</li></ul><p><strong>Returns</strong></p><ul><li><code>(evaluation, gradient, complete)::Tuple{AbstractVector{T}, AbstractMatrix{T}, Bool}</code>: the normal evaluation,   the gradient, and whether the evaluation completed as normal (or encountered a nan or inf).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/EvaluateEquationDerivative.jl#L119-L140">source</a></section></article><h2 id="SymbolicUtils.jl-interface"><a class="docs-heading-anchor" href="#SymbolicUtils.jl-interface">SymbolicUtils.jl interface</a><a id="SymbolicUtils.jl-interface-1"></a><a class="docs-heading-anchor-permalink" href="#SymbolicUtils.jl-interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.InterfaceSymbolicUtilsModule.node_to_symbolic-Tuple{Node, Options}" href="#SymbolicRegression.InterfaceSymbolicUtilsModule.node_to_symbolic-Tuple{Node, Options}"><code>SymbolicRegression.InterfaceSymbolicUtilsModule.node_to_symbolic</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">node_to_symbolic(tree::Node, options::Options;
            varMap::Union{Array{String, 1}, Nothing}=nothing,
            index_functions::Bool=false)</code></pre><p>The interface to SymbolicUtils.jl. Passing a tree to this function will generate a symbolic equation in SymbolicUtils.jl format.</p><p><strong>Arguments</strong></p><ul><li><code>tree::Node</code>: The equation to convert.</li><li><code>options::Options</code>: Options, which contains the operators used in the equation.</li><li><code>varMap::Union{Array{String, 1}, Nothing}=nothing</code>: What variable names to use for   each feature. Default is [x1, x2, x3, ...].</li><li><code>index_functions::Bool=false</code>: Whether to generate special names for the   operators, which then allows one to convert back to a <code>Node</code> format   using <code>symbolic_to_node</code>.   (CURRENTLY UNAVAILABLE - See https://github.com/MilesCranmer/SymbolicRegression.jl/pull/84).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/InterfaceSymbolicUtils.jl#L137-L155">source</a></section></article><h2 id="Pareto-frontier"><a class="docs-heading-anchor" href="#Pareto-frontier">Pareto frontier</a><a id="Pareto-frontier-1"></a><a class="docs-heading-anchor-permalink" href="#Pareto-frontier" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.HallOfFameModule.calculate_pareto_frontier-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, HallOfFame{T}, Options}} where T&lt;:Real" href="#SymbolicRegression.HallOfFameModule.calculate_pareto_frontier-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, HallOfFame{T}, Options}} where T&lt;:Real"><code>SymbolicRegression.HallOfFameModule.calculate_pareto_frontier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">calculate_pareto_frontier(X::AbstractMatrix{T}, y::AbstractVector{T},
                        hallOfFame::HallOfFame{T}, options::Options;
                        weights=nothing, varMap=nothing) where {T&lt;:Real}</code></pre><p>Compute the dominating Pareto frontier for a given hallOfFame. This is the list of equations where each equation has a better loss than all simpler equations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/HallOfFame.jl#L82-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SymbolicRegression.HallOfFameModule.calculate_pareto_frontier-Union{Tuple{T}, Tuple{Dataset{T}, HallOfFame{T}, Options}} where T&lt;:Real" href="#SymbolicRegression.HallOfFameModule.calculate_pareto_frontier-Union{Tuple{T}, Tuple{Dataset{T}, HallOfFame{T}, Options}} where T&lt;:Real"><code>SymbolicRegression.HallOfFameModule.calculate_pareto_frontier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">calculate_pareto_frontier(dataset::Dataset{T}, hallOfFame::HallOfFame{T},
                        options::Options) where {T&lt;:Real}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MilesCranmer/SymbolicRegression.jl/blob/b6abb85136e465ab4d01aac81013958345bd0237/src/HallOfFame.jl#L46-L49">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« SymbolicRegression.jl</a><a class="docs-footer-nextpage" href="../losses/">Losses »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 5 October 2022 23:55">Wednesday 5 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
