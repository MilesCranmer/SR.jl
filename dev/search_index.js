var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#equation_search","page":"API","title":"equation_search","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"equation_search(X::AbstractMatrix{T}, y::AbstractMatrix{T};\n        niterations::Int=10,\n        weights::Union{AbstractVector{T}, Nothing}=nothing,\n        varMap::Union{Array{String, 1}, Nothing}=nothing,\n        options::Options=Options(),\n        numprocs::Union{Int, Nothing}=nothing,\n        procs::Union{Array{Int, 1}, Nothing}=nothing,\n        runtests::Bool=true,\n        loss_type::Type=Nothing,\n) where {T<:DATA_TYPE}","category":"page"},{"location":"api/#SymbolicRegression.equation_search-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractMatrix{T}}} where T<:Number","page":"API","title":"SymbolicRegression.equation_search","text":"equation_search(X, y[; kws...])\n\nPerform a distributed equation search for functions f_i which describe the mapping f_i(X[:, j]) ≈ y[i, j]. Options are configured using SymbolicRegression.Options(...), which should be passed as a keyword argument to options. One can turn off parallelism with numprocs=0, which is useful for debugging and profiling.\n\nArguments\n\nX::AbstractMatrix{T}:  The input dataset to predict y from.   The first dimension is features, the second dimension is rows.\ny::Union{AbstractMatrix{T}, AbstractVector{T}}: The values to predict. The first dimension   is the output feature to predict with each equation, and the   second dimension is rows.\nniterations::Int=10: The number of iterations to perform the search.   More iterations will improve the results.\nweights::Union{AbstractMatrix{T}, AbstractVector{T}, Nothing}=nothing: Optionally   weight the loss for each y by this value (same shape as y).\nvariable_names::Union{Vector{String}, Nothing}=nothing: The names   of each feature in X, which will be used during printing of equations.\noptions::Options=Options(): The options for the search, such as   which operators to use, evolution hyperparameters, etc.\nparallelism=:multithreading: What parallelism mode to use.   The options are :multithreading, :multiprocessing, and :serial.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using :multithreading   mode, the number of threads available to julia are used. If using   :multiprocessing, numprocs processes will be created dynamically if   procs is unset. If you have already allocated processes, pass them   to the procs argument and they will be used.   You may also pass a string instead of a symbol, like \"multithreading\".\nnumprocs::Union{Int, Nothing}=nothing:  The number of processes to use,   if you want equation_search to set this up automatically. By default   this will be 4, but can be any number (you should pick a number <=   the number of cores available).\nprocs::Union{Vector{Int}, Nothing}=nothing: If you have set up   a distributed run manually with procs = addprocs() and @everywhere,   pass the procs to this keyword argument.\naddprocs_function::Union{Function, Nothing}=nothing: If using multiprocessing   (parallelism=:multithreading), and are not passing procs manually,   then they will be allocated dynamically using addprocs. However,   you may also pass a custom function to use instead of addprocs.   This function should take a single positional argument,   which is the number of processes to use, as well as the lazy keyword argument.   For example, if set up on a slurm cluster, you could pass   addprocs_function = addprocs_slurm, which will set up slurm processes.\nruntests::Bool=true: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.\nsaved_state::Union{StateType, Nothing}=nothing: If you have already   run equation_search and want to resume it, pass the state here.   To get this to work, you need to have set returnstate=true,   which will cause `equationsearch` to return the state. Note that   you cannot change the operators or dataset, but most other options   should be changeable.\nreturn_state::Union{Bool, Nothing}=nothing: Whether to return the   state of the search for warm starts. By default this is false.\nloss_type::Type=Nothing: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data ::Complex{L}, then the loss   type will automatically be set to L.\n\nReturns\n\nhallOfFame::HallOfFame: The best equations seen during the search.   hallOfFame.members gives an array of PopMember objects, which   have their tree (equation) stored in .tree. Their score (loss)   is given in .score. The array of PopMember objects   is enumerated by size from 1 to options.maxsize.\n\n\n\n\n\n","category":"method"},{"location":"api/#Options","page":"API","title":"Options","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Options(;)\nMutationWeights(;)","category":"page"},{"location":"api/#SymbolicRegression.CoreModule.OptionsStructModule.MutationWeights-Tuple{}","page":"API","title":"SymbolicRegression.CoreModule.OptionsStructModule.MutationWeights","text":"MutationWeights(;kws...)\n\nThis defines how often different mutations occur. These weightings will be normalized to sum to 1.0 after initialization.\n\nArguments\n\nmutate_constant::Float64: How often to mutate a constant.\nmutate_operator::Float64: How often to mutate an operator.\nadd_node::Float64: How often to append a node to the tree.\ninsert_node::Float64: How often to insert a node into the tree.\ndelete_node::Float64: How often to delete a node from the tree.\nsimplify::Float64: How often to simplify the tree.\nrandomize::Float64: How often to create a random tree.\ndo_nothing::Float64: How often to do nothing.\noptimize::Float64: How often to optimize the constants in the tree, as a mutation. Note that this is different from optimizer_probability, which is performed at the end of an iteration for all individuals.\n\n\n\n\n\n","category":"method"},{"location":"api/#Printing","page":"API","title":"Printing","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"string_tree(tree::Node, options::Options; kws...)","category":"page"},{"location":"api/#DynamicExpressions.EquationModule.string_tree-Tuple{Node, Options}","page":"API","title":"DynamicExpressions.EquationModule.string_tree","text":"string_tree(tree::Node, options::Options; kws...)\n\nConvert an equation to a string.\n\nArguments\n\ntree::Node: The equation to convert to a string.\noptions::Options: The options holding the definition of operators.\nvariable_names::Union{Array{String, 1}, Nothing}=nothing: what variables   to print for each feature.\n\n\n\n\n\n","category":"method"},{"location":"api/#Evaluation","page":"API","title":"Evaluation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"eval_tree_array(tree::Node, X::AbstractMatrix, options::Options; kws...)","category":"page"},{"location":"api/#DynamicExpressions.EvaluateEquationModule.eval_tree_array-Tuple{Node, AbstractMatrix, Options}","page":"API","title":"DynamicExpressions.EvaluateEquationModule.eval_tree_array","text":"eval_tree_array(tree::Node, X::AbstractArray, options::Options; kws...)\n\nEvaluate a binary tree (equation) over a given input data matrix. The operators contain all of the operators used. This function fuses doublets and triplets of operations for lower memory usage.\n\nThis function can be represented by the following pseudocode:\n\nfunction eval(current_node)\n    if current_node is leaf\n        return current_node.value\n    elif current_node is degree 1\n        return current_node.operator(eval(current_node.left_child))\n    else\n        return current_node.operator(eval(current_node.left_child), eval(current_node.right_child))\n\nThe bulk of the code is for optimizations and pre-emptive NaN/Inf checks, which speed up evaluation significantly.\n\nArguments\n\ntree::Node: The root node of the tree to evaluate.\nX::AbstractArray: The input data to evaluate the tree on.\noptions::Options: Options used to define the operators used in the tree.\n\nReturns\n\n(output, complete)::Tuple{AbstractVector, Bool}: the result,   which is a 1D array, as well as if the evaluation completed   successfully (true/false). A false complete means an infinity   or nan was encountered, and a large loss should be assigned   to the equation.\n\n\n\n\n\n","category":"method"},{"location":"api/#Derivatives","page":"API","title":"Derivatives","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SymbolicRegression.jl can automatically and efficiently compute derivatives of expressions with respect to variables or constants. This is done using either eval_diff_tree_array, to compute derivative with respect to a single variable, or with eval_grad_tree_array, to compute the gradient with respect all variables (or, all constants). Both use forward-mode automatic, but use Zygote.jl to compute derivatives of each operator, so this is very efficient.","category":"page"},{"location":"api/","page":"API","title":"API","text":"eval_diff_tree_array(tree::Node, X::AbstractMatrix, options::Options, direction::Int)\neval_grad_tree_array(tree::Node, X::AbstractMatrix, options::Options; kws...)","category":"page"},{"location":"api/#DynamicExpressions.EvaluateEquationDerivativeModule.eval_diff_tree_array-Tuple{Node, AbstractMatrix, Options, Int64}","page":"API","title":"DynamicExpressions.EvaluateEquationDerivativeModule.eval_diff_tree_array","text":"eval_diff_tree_array(tree::Node, X::AbstractArray, options::Options, direction::Int)\n\nCompute the forward derivative of an expression, using a similar structure and optimization to evaltreearray. direction is the index of a particular variable in the expression. e.g., direction=1 would indicate derivative with respect to x1.\n\nArguments\n\ntree::Node: The expression tree to evaluate.\nX::AbstractArray: The data matrix, with each column being a data point.\noptions::Options: The options containing the operators used to create the tree.   enable_autodiff must be set to true when creating the options.   This is needed to create the derivative operations.\ndirection::Int: The index of the variable to take the derivative with respect to.\n\nReturns\n\n(evaluation, derivative, complete)::Tuple{AbstractVector, AbstractVector, Bool}: the normal evaluation,   the derivative, and whether the evaluation completed as normal (or encountered a nan or inf).\n\n\n\n\n\n","category":"method"},{"location":"api/#DynamicExpressions.EvaluateEquationDerivativeModule.eval_grad_tree_array-Tuple{Node, AbstractMatrix, Options}","page":"API","title":"DynamicExpressions.EvaluateEquationDerivativeModule.eval_grad_tree_array","text":"eval_grad_tree_array(tree::Node, X::AbstractArray, options::Options; variable::Bool=false)\n\nCompute the forward-mode derivative of an expression, using a similar structure and optimization to evaltreearray. variable specifies whether we should take derivatives with respect to features (i.e., X), or with respect to every constant in the expression.\n\nArguments\n\ntree::Node: The expression tree to evaluate.\nX::AbstractArray: The data matrix, with each column being a data point.\noptions::Options: The options containing the operators used to create the tree.   enable_autodiff must be set to true when creating the options.   This is needed to create the derivative operations.\nvariable::Bool: Whether to take derivatives with respect to features (i.e., X - with variable=true),   or with respect to every constant in the expression (variable=false).\n\nReturns\n\n(evaluation, gradient, complete)::Tuple{AbstractVector, AbstractArray, Bool}: the normal evaluation,   the gradient, and whether the evaluation completed as normal (or encountered a nan or inf).\n\n\n\n\n\n","category":"method"},{"location":"api/#SymbolicUtils.jl-interface","page":"API","title":"SymbolicUtils.jl interface","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"node_to_symbolic(tree::Node, options::Options; \n                     varMap::Union{Array{String, 1}, Nothing}=nothing,\n                     index_functions::Bool=false)","category":"page"},{"location":"api/#DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic-Tuple{Node, Options}","page":"API","title":"DynamicExpressions.ExtensionInterfaceModule.node_to_symbolic","text":"node_to_symbolic(tree::Node, options::Options; kws...)\n\nConvert an expression to SymbolicUtils.jl form. \n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"Note that use of this function requires SymbolicUtils.jl to be installed and loaded.","category":"page"},{"location":"api/#Pareto-frontier","page":"API","title":"Pareto frontier","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"calculate_pareto_frontier(hallOfFame::HallOfFame{T,L}) where {T<:DATA_TYPE,L<:LOSS_TYPE}","category":"page"},{"location":"api/#SymbolicRegression.HallOfFameModule.calculate_pareto_frontier-Union{Tuple{HallOfFame{T, L}}, Tuple{L}, Tuple{T}} where {T<:Number, L<:Real}","page":"API","title":"SymbolicRegression.HallOfFameModule.calculate_pareto_frontier","text":"calculate_pareto_frontier(hallOfFame::HallOfFame{T,L}) where {T<:DATA_TYPE,L<:LOSS_TYPE}\n\n\n\n\n\n","category":"method"},{"location":"index_base/#Contents","page":"Contents","title":"Contents","text":"","category":"section"},{"location":"index_base/","page":"Contents","title":"Contents","text":"Pages = [\"examples.md\", \"api.md\", \"types.md\", \"losses.md\"]","category":"page"},{"location":"examples/#Toy-Examples-with-Code","page":"Examples","title":"Toy Examples with Code","text":"","category":"section"},{"location":"examples/#Preamble","page":"Examples","title":"Preamble","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using SymbolicRegression\nusing DataFrames","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We'll also code up a simple function to print a single expression:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function get_best(; hof::HallOfFame{T,L}, options) where {T,L}\n    dominating = calculate_pareto_frontier(hof)\n\n    df = DataFrame(;\n        tree=[m.tree for m in dominating],\n        loss=[m.loss for m in dominating],\n        complexity=[compute_complexity(m, options) for m in dominating],\n    )\n\n    df[!, :score] = vcat(\n        [L(0.0)],\n        -1 .* log.(df.loss[2:end] ./ df.loss[1:(end - 1)]) ./\n        (df.complexity[2:end] .- df.complexity[1:(end - 1)]),\n    )\n\n    min_loss = min(df.loss...)\n\n    best_idx = argmax(df.score .* (df.loss .<= (2 * min_loss)))\n\n    return df.tree[best_idx], df\nend","category":"page"},{"location":"examples/#.-Simple-search","page":"Examples","title":"1. Simple search","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Here's a simple example where we find the expression 2 cos(x3) + x0^2 - 2.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"X = 2randn(5, 1000)\ny = @. 2*cos(X[4, :]) + X[1, :]^2 - 2\n\noptions = Options(; binary_operators=[+, -, *, /], unary_operators=[cos])\nhof = equation_search(X, y; options=options, niterations=30)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's look at the most accurate expression:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"best, df = get_best(; hof, options)\nprintln(best)","category":"page"},{"location":"examples/#.-Custom-operator","page":"Examples","title":"2. Custom operator","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Here, we define a custom operator and use it to find an expression:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"X = 2randn(5, 1000)\ny = @. 1/X[1, :]\n\noptions = Options(; binary_operators=[+, *], unary_operators=[inv])\nhof = equation_search(X, y; options=options)\nprintln(get_best(; hof, options)[1])","category":"page"},{"location":"examples/#.-Multiple-outputs","page":"Examples","title":"3. Multiple outputs","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Here, we do the same thing, but with multiple expressions at once, each requiring a different feature.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"X = 2rand(5, 1000) .+ 0.1\ny = @. 1/X[1:3, :]\noptions = Options(; binary_operators=[+, *], unary_operators=[inv])\nhofs = equation_search(X, y; options=options)\nbests = [get_best(; hof=hofs[i], options)[1] for i=1:3]\nprintln(bests)","category":"page"},{"location":"examples/#.-Plotting-an-expression","page":"Examples","title":"4. Plotting an expression","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"For now, let's consider the expressions for output 1. We can see the SymbolicUtils version with:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using SymbolicUtils\n\neqn = node_to_symbolic(bests[1], options)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We can get the LaTeX version with:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Latexify\nlatexify(string(eqn))","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's plot the prediction against the truth:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Plots\n\nscatter(y[1, :], bests[1](X), xlabel=\"Truth\", ylabel=\"Prediction\")","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Here, we have used the convenience function (::Node{T})(X) to evaluate the expression. However, this will only work because calling Options() will automatically set up calls to eval_tree_array. In practice, you should use the eval_tree_array function directly, which is the form of:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"eval_tree_array(bests[1], X, options)","category":"page"},{"location":"examples/#.-Other-types","page":"Examples","title":"5. Other types","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"SymbolicRegression.jl can handle most numeric types you wish to use. For example, passing a Float32 array will result in the search using 32-bit precision everywhere in the codebase:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"X = 2randn(Float32, 5, 1000)\ny = @. 2*cos(X[4, :]) + X[1, :]^2 - 2\n\noptions = Options(; binary_operators=[+, -, *, /], unary_operators=[cos])\nhof = equation_search(X, y; options=options, niterations=30)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"we can see that the output types are Float32:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"best, df = get_best(; hof, options)\nprintln(typeof(best))\n# Node{Float32}","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We can also use Complex numbers:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"cos_re(x::Complex{T}) where {T} = cos(abs(x)) + 0im\n\nX = 15 .* rand(ComplexF64, 5, 1000) .- 7.5\ny = @. 2*cos_re((2+1im) * X[4, :]) + 0.1 * X[1, :]^2 - 2\n\noptions = Options(; binary_operators=[+, -, *, /], unary_operators=[cos_re], maxsize=30)\nhof = equation_search(X, y; options=options, niterations=100)","category":"page"},{"location":"examples/#.-Additional-features","page":"Examples","title":"6. Additional features","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"For the many other features available in SymbolicRegression.jl, check out the API page for Options. You might also find it useful to browse the documentation for the Python frontend PySR, which has additional documentation. In particular, the tuning page is useful for improving search performance.","category":"page"},{"location":"losses/#Losses","page":"Losses","title":"Losses","text":"","category":"section"},{"location":"losses/","page":"Losses","title":"Losses","text":"These losses, and their documentation, are included from the LossFunctions.jl package.","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"Pass the function as, e.g., elementwise_loss=L1DistLoss().","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"You can also declare your own loss as a function that takes two (unweighted) or three (weighted) scalar arguments. For example,","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"f(x, y, w) = abs(x-y)*w\noptions = Options(elementwise_loss=f)","category":"page"},{"location":"losses/#Regression","page":"Losses","title":"Regression","text":"","category":"section"},{"location":"losses/","page":"Losses","title":"Losses","text":"Regression losses work on the distance between targets and predictions: r = x - y.","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"LPDistLoss{P}\nL1DistLoss\nL2DistLoss\nPeriodicLoss\nHuberLoss\nL1EpsilonInsLoss\nL2EpsilonInsLoss\nLogitDistLoss\nQuantileLoss","category":"page"},{"location":"losses/#LossFunctions.LPDistLoss","page":"Losses","title":"LossFunctions.LPDistLoss","text":"LPDistLoss{P} <: DistanceLoss\n\nThe P-th power absolute distance loss. It is Lipschitz continuous iff P == 1, convex if and only if P >= 1, and strictly convex iff P > 1.\n\nL(r) = r^P\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L1DistLoss","page":"Losses","title":"LossFunctions.L1DistLoss","text":"L1DistLoss <: DistanceLoss\n\nThe absolute distance loss. Special case of the LPDistLoss with P=1. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(r) = r\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    3 │\\.                     ./│    1 │            ┌------------│\n      │ '\\.                 ./' │      │            |            │\n      │   \\.               ./   │      │            |            │\n      │    '\\.           ./'    │      │_           |           _│\n    L │      \\.         ./      │   L' │            |            │\n      │       '\\.     ./'       │      │            |            │\n      │         \\.   ./         │      │            |            │\n    0 │          '\\./'          │   -1 │------------┘            │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -3                        3\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2DistLoss","page":"Losses","title":"LossFunctions.L2DistLoss","text":"L2DistLoss <: DistanceLoss\n\nThe least squares loss. Special case of the LPDistLoss with P=2. It is strictly convex.\n\nL(r) = r^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    9 │\\                       /│    3 │                   .r/   │\n      │\".                     .\"│      │                 .r'     │\n      │ \".                   .\" │      │              _./'       │\n      │  \".                 .\"  │      │_           .r/         _│\n    L │   \".               .\"   │   L' │         _:/'            │\n      │    '\\.           ./'    │      │       .r'               │\n      │      \\.         ./      │      │     .r'                 │\n    0 │        \"-.___.-\"        │   -3 │  _/r'                   │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.PeriodicLoss","page":"Losses","title":"LossFunctions.PeriodicLoss","text":"PeriodicLoss <: DistanceLoss\n\nMeasures distance on a circle of specified circumference c.\n\nL(r) = 1 - cos left( frac2 r pic right)\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.HuberLoss","page":"Losses","title":"LossFunctions.HuberLoss","text":"HuberLoss <: DistanceLoss\n\nLoss function commonly used for robustness to outliers. For large values of d it becomes close to the L1DistLoss, while for small values of d it resembles the L2DistLoss. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(r) = begincases fracr^22  quad textif   r  le alpha  alpha  r  - fracalpha^32  quad textotherwise endcases\n\n\n\n              Lossfunction (d=1)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │                         │    1 │                .+-------│\n      │                         │      │              ./'        │\n      │\\.                     ./│      │             ./          │\n      │ '.                   .' │      │_           ./          _│\n    L │   \\.               ./   │   L' │           /'            │\n      │     \\.           ./     │      │          /'             │\n      │      '.         .'      │      │        ./'              │\n    0 │        '-.___.-'        │   -1 │-------+'                │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L1EpsilonInsLoss","page":"Losses","title":"LossFunctions.L1EpsilonInsLoss","text":"L1EpsilonInsLoss <: DistanceLoss\n\nThe ϵ-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than ϵ, but penalizes larger deviances linarily. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(r) = max  0  r  - epsilon \n\n\n\n              Lossfunction (ϵ=1)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\\                       /│    1 │                  ┌------│\n      │ \\                     / │      │                  |      │\n      │  \\                   /  │      │                  |      │\n      │   \\                 /   │      │_      ___________!     _│\n    L │    \\               /    │   L' │      |                  │\n      │     \\             /     │      │      |                  │\n      │      \\           /      │      │      |                  │\n    0 │       \\_________/       │   -1 │------┘                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2EpsilonInsLoss","page":"Losses","title":"LossFunctions.L2EpsilonInsLoss","text":"L2EpsilonInsLoss <: DistanceLoss\n\nThe quadratic ϵ-insensitive loss. Typically used in linear support vector regression. It ignores deviances smaller than ϵ, but penalizes larger deviances quadratically. It is convex, but not strictly convex.\n\nL(r) = max  0  r  - epsilon ^2\n\n\n\n              Lossfunction (ϵ=0.5)             Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    8 │                         │    1 │                  /      │\n      │:                       :│      │                 /       │\n      │'.                     .'│      │                /        │\n      │ \\.                   ./ │      │_         _____/        _│\n    L │  \\.                 ./  │   L' │         /               │\n      │   \\.               ./   │      │        /                │\n      │    '\\.           ./'    │      │       /                 │\n    0 │      '-._______.-'      │   -1 │      /                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -2                        2\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.LogitDistLoss","page":"Losses","title":"LossFunctions.LogitDistLoss","text":"LogitDistLoss <: DistanceLoss\n\nThe distance-based logistic loss for regression. It is strictly convex and Lipschitz continuous.\n\nL(r) = - ln frac4 e^r(1 + e^r)^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │                         │    1 │                   _--'''│\n      │\\                       /│      │                ./'      │\n      │ \\.                   ./ │      │              ./         │\n      │  '.                 .'  │      │_           ./          _│\n    L │   '.               .'   │   L' │           ./            │\n      │     \\.           ./     │      │         ./              │\n      │      '.         .'      │      │       ./                │\n    0 │        '-.___.-'        │   -1 │___.-''                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -4                        4\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.QuantileLoss","page":"Losses","title":"LossFunctions.QuantileLoss","text":"QuantileLoss <: DistanceLoss\n\nThe distance-based quantile loss, also known as pinball loss, can be used to estimate conditional τ-quantiles. It is Lipschitz continuous and convex, but not strictly convex. Furthermore it is symmetric if and only if τ = 1/2.\n\nL(r) = begincases -left( 1 - tau  right) r  quad textif  r  0  tau r  quad textif  r ge 0  endcases\n\n\n\n              Lossfunction (τ=0.7)             Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │'\\                       │  0.3 │            ┌------------│\n      │  \\.                     │      │            |            │\n      │   '\\                    │      │_           |           _│\n      │     \\.                  │      │            |            │\n    L │      '\\              ._-│   L' │            |            │\n      │        \\.         ..-'  │      │            |            │\n      │         '.     _r/'     │      │            |            │\n    0 │           '_./'         │ -0.7 │------------┘            │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -3                        3      -3                        3\n                 ŷ - y                            ŷ - y\n\n\n\n","category":"type"},{"location":"losses/#Classification","page":"Losses","title":"Classification","text":"","category":"section"},{"location":"losses/","page":"Losses","title":"Losses","text":"Classifications losses (assuming binary) work on the margin between targets and predictions: r = x y, assuming the target y is either -1 or +1.","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"ZeroOneLoss\nPerceptronLoss\nLogitMarginLoss\nL1HingeLoss\nL2HingeLoss\nSmoothedL1HingeLoss\nModifiedHuberLoss\nL2MarginLoss\nExpLoss\nSigmoidLoss\nDWDMarginLoss","category":"page"},{"location":"losses/#LossFunctions.ZeroOneLoss","page":"Losses","title":"LossFunctions.ZeroOneLoss","text":"ZeroOneLoss <: MarginLoss\n\nThe classical classification loss. It penalizes every misclassified observation with a loss of 1 while every correctly classified observation has a loss of 0. It is not convex nor continuous and thus seldom used directly. Instead one usually works with some classification-calibrated surrogate loss, such as L1HingeLoss.\n\nL(a) = begincases 1  quad textif  a  0  0  quad textif  a = 0 endcases\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    1 │------------┐            │    1 │                         │\n      │            |            │      │                         │\n      │            |            │      │                         │\n      │            |            │      │_________________________│\n      │            |            │      │                         │\n      │            |            │      │                         │\n      │            |            │      │                         │\n    0 │            └------------│   -1 │                         │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                y * h(x)                         y * h(x)\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.PerceptronLoss","page":"Losses","title":"LossFunctions.PerceptronLoss","text":"PerceptronLoss <: MarginLoss\n\nThe perceptron loss linearly penalizes every prediction where the resulting agreement <= 0. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = max  0 -a \n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\\.                       │    0 │            ┌------------│\n      │ '..                     │      │            |            │\n      │   \\.                    │      │            |            │\n      │     '.                  │      │            |            │\n    L │      '.                 │   L' │            |            │\n      │        \\.               │      │            |            │\n      │         '.              │      │            |            │\n    0 │           \\.____________│   -1 │------------┘            │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.LogitMarginLoss","page":"Losses","title":"LossFunctions.LogitMarginLoss","text":"LogitMarginLoss <: MarginLoss\n\nThe margin version of the logistic loss. It is infinitely many times differentiable, strictly convex, and Lipschitz continuous.\n\nL(a) = ln (1 + e^-a)\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │ \\.                      │    0 │                  ._--/\"\"│\n      │   \\.                    │      │               ../'      │\n      │     \\.                  │      │              ./         │\n      │       \\..               │      │            ./'          │\n    L │         '-_             │   L' │          .,'            │\n      │            '-_          │      │         ./              │\n      │               '\\-._     │      │      .,/'               │\n    0 │                    '\"\"*-│   -1 │__.--''                  │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -4                        4\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L1HingeLoss","page":"Losses","title":"LossFunctions.L1HingeLoss","text":"L1HingeLoss <: MarginLoss\n\nThe hinge loss linearly penalizes every predicition where the resulting agreement < 1 . It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = max  0 1 - a \n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    3 │'\\.                      │    0 │                  ┌------│\n      │  ''_                    │      │                  |      │\n      │     \\.                  │      │                  |      │\n      │       '.                │      │                  |      │\n    L │         ''_             │   L' │                  |      │\n      │            \\.           │      │                  |      │\n      │              '.         │      │                  |      │\n    0 │                ''_______│   -1 │------------------┘      │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2HingeLoss","page":"Losses","title":"LossFunctions.L2HingeLoss","text":"L2HingeLoss <: MarginLoss\n\nThe truncated least squares loss quadratically penalizes every predicition where the resulting agreement < 1. It is locally Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = max  0 1 - a ^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │     .                   │    0 │                 ,r------│\n      │     '.                  │      │               ,/        │\n      │      '\\                 │      │             ,/          │\n      │        \\                │      │           ,/            │\n    L │         '.              │   L' │         ./              │\n      │          '.             │      │       ./                │\n      │            \\.           │      │     ./                  │\n    0 │              '-.________│   -5 │   ./                    │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.SmoothedL1HingeLoss","page":"Losses","title":"LossFunctions.SmoothedL1HingeLoss","text":"SmoothedL1HingeLoss <: MarginLoss\n\nAs the name suggests a smoothed version of the L1 hinge loss. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = begincases frac05gamma cdot max  0 1 - a  ^2  quad textif  a ge 1 - gamma  1 - fracgamma2 - a  quad textotherwise endcases\n\n\n\n              Lossfunction (γ=2)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\\.                       │    0 │                 ,r------│\n      │ '.                      │      │               ./'       │\n      │   \\.                    │      │              ,/         │\n      │     '.                  │      │            ./'          │\n    L │      '.                 │   L' │           ,'            │\n      │        \\.               │      │         ,/              │\n      │          ',             │      │       ./'               │\n    0 │            '*-._________│   -1 │______./                 │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.ModifiedHuberLoss","page":"Losses","title":"LossFunctions.ModifiedHuberLoss","text":"ModifiedHuberLoss <: MarginLoss\n\nA special (4 times scaled) case of the SmoothedL1HingeLoss with γ=2. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = begincases max  0 1 - a  ^2  quad textif  a ge -1  - 4 a  quad textotherwise endcases\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │    '.                   │    0 │                .+-------│\n      │     '.                  │      │              ./'        │\n      │      '\\                 │      │             ,/          │\n      │        \\                │      │           ,/            │\n    L │         '.              │   L' │         ./              │\n      │          '.             │      │       ./'               │\n      │            \\.           │      │______/'                 │\n    0 │              '-.________│   -5 │                         │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.L2MarginLoss","page":"Losses","title":"LossFunctions.L2MarginLoss","text":"L2MarginLoss <: MarginLoss\n\nThe margin-based least-squares loss for classification, which penalizes every prediction where agreement != 1 quadratically. It is locally Lipschitz continuous and strongly convex.\n\nL(a) = left( 1 - a right)^2\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │     .                   │    2 │                       ,r│\n      │     '.                  │      │                     ,/  │\n      │      '\\                 │      │                   ,/    │\n      │        \\                │      ├                 ,/      ┤\n    L │         '.              │   L' │               ./        │\n      │          '.             │      │             ./          │\n      │            \\.          .│      │           ./            │\n    0 │              '-.____.-' │   -3 │         ./              │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.ExpLoss","page":"Losses","title":"LossFunctions.ExpLoss","text":"ExpLoss <: MarginLoss\n\nThe margin-based exponential loss for classification, which penalizes every prediction exponentially. It is infinitely many times differentiable, locally Lipschitz continuous and strictly convex, but not clipable.\n\nL(a) = e^-a\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    5 │  \\.                     │    0 │               _,,---:'\"\"│\n      │   l                     │      │           _r/\"'         │\n      │    l.                   │      │        .r/'             │\n      │     \":                  │      │      .r'                │\n    L │       \\.                │   L' │     ./                  │\n      │        \"\\..             │      │    .'                   │\n      │           '\":,_         │      │   ,'                    │\n    0 │                \"\"---:.__│   -5 │  ./                     │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.SigmoidLoss","page":"Losses","title":"LossFunctions.SigmoidLoss","text":"SigmoidLoss <: MarginLoss\n\nContinuous loss which penalizes every prediction with a loss within in the range (0,2). It is infinitely many times differentiable, Lipschitz continuous but nonconvex.\n\nL(a) = 1 - tanh(a)\n\n\n\n              Lossfunction                     Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │\"\"'--,.                  │    0 │..                     ..│\n      │      '\\.                │      │ \"\\.                 ./\" │\n      │         '.              │      │    ',             ,'    │\n      │           \\.            │      │      \\           /      │\n    L │            \"\\.          │   L' │       \\         /       │\n      │              \\.         │      │        \\.     ./        │\n      │                \\,       │      │         \\.   ./         │\n    0 │                  '\"-:.__│   -1 │          ',_,'          │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"losses/#LossFunctions.DWDMarginLoss","page":"Losses","title":"LossFunctions.DWDMarginLoss","text":"DWDMarginLoss <: MarginLoss\n\nThe distance weighted discrimination margin loss. It is a differentiable generalization of the L1HingeLoss that is different than the SmoothedL1HingeLoss. It is Lipschitz continuous and convex, but not strictly convex.\n\nL(a) = begincases 1 - a  quad textif  a ge fracqq+1  frac1a^q fracq^q(q+1)^q+1  quad textotherwise endcases\n\n\n\n              Lossfunction (q=1)               Derivative\n      ┌────────────┬────────────┐      ┌────────────┬────────────┐\n    2 │      \".                 │    0 │                     ._r-│\n      │        \\.               │      │                   ./    │\n      │         ',              │      │                 ./      │\n      │           \\.            │      │                 /       │\n    L │            \"\\.          │   L' │                .        │\n      │              \\.         │      │                /        │\n      │               \":__      │      │               ;         │\n    0 │                   '\"\"---│   -1 │---------------┘         │\n      └────────────┴────────────┘      └────────────┴────────────┘\n      -2                        2      -2                        2\n                 y ⋅ ŷ                            y ⋅ ŷ\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"<script type=\"module\">\n  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@9/dist/mermaid.esm.min.mjs';\n  mermaid.initialize({ startOnLoad: true });\n</script>","category":"page"},{"location":"","page":"Home","title":"Home","text":"SymbolicRegression.jl searches for symbolic expressions which optimize a particular objective.","category":"page"},{"location":"","page":"Home","title":"Home","text":"https://github.com/MilesCranmer/SymbolicRegression.jl/assets/7593028/f5b68f1f-9830-497f-a197-6ae332c94ee0","category":"page"},{"location":"","page":"Home","title":"Home","text":"Latest release Documentation Forums Paper\n(Image: version) (Image: Dev) (Image: Discussions) (Image: Paper)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Build status Coverage\n(Image: CI) (Image: Coverage Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check out PySR for a Python frontend.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: ) (Image: )","category":"page"},{"location":"","page":"Home","title":"Home","text":"Cite this software","category":"page"},{"location":"#Quickstart","page":"Home","title":"Quickstart","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install in Julia with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"SymbolicRegression\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"The heart of this package is the equation_search function, which takes a 2D array (shape [features, rows]) and attempts to model a 1D array (shape [rows]) using analytic functional forms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Run with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SymbolicRegression\n\nX = randn(Float32, 5, 100)\ny = 2 * cos.(X[4, :]) + X[1, :] .^ 2 .- 2\n\noptions = SymbolicRegression.Options(\n    binary_operators=[+, *, /, -],\n    unary_operators=[cos, exp],\n    npopulations=20\n)\n\nhall_of_fame = equation_search(\n    X, y, niterations=40, options=options,\n    parallelism=:multithreading\n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can view the resultant equations in the dominating Pareto front (best expression seen at each complexity) with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"dominating = calculate_pareto_frontier(hall_of_fame)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is a vector of PopMember type - which contains the expression along with the score. We can get the expressions with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"trees = [member.tree for member in dominating]","category":"page"},{"location":"","page":"Home","title":"Home","text":"Each of these equations is a Node{T} type for some constant type T (like Float32).","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can evaluate a given tree with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"tree = trees[end]\noutput, did_succeed = eval_tree_array(tree, X, options)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The output array will contain the result of the tree at each of the 100 rows. This did_succeed flag detects whether an evaluation was successful, or whether encountered any NaNs or Infs during calculation (such as, e.g., sqrt(-1)).","category":"page"},{"location":"#Constructing-trees","page":"Home","title":"Constructing trees","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can also manipulate and construct trees directly. For example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SymbolicRegression\n\noptions = Options(;\n    binary_operators=[+, -, *, ^, /], unary_operators=[cos, exp, sin]\n)\nx1, x2, x3 = [Node(; feature=i) for i=1:3]\ntree = cos(x1 - 3.2 * x2) - x1^3.2","category":"page"},{"location":"","page":"Home","title":"Home","text":"This tree has Float64 constants, so the type of the entire tree will be promoted to Node{Float64}.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can convert all constants (recursively) to Float32:","category":"page"},{"location":"","page":"Home","title":"Home","text":"float32_tree = convert(Node{Float32}, tree)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can then evaluate this tree on a dataset:","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = rand(Float32, 3, 100)\noutput, did_succeed = eval_tree_array(tree, X, options)","category":"page"},{"location":"#Exporting-to-SymbolicUtils.jl","page":"Home","title":"Exporting to SymbolicUtils.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We can view the equations in the dominating Pareto frontier with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"dominating = calculate_pareto_frontier(hall_of_fame)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can convert the best equation to SymbolicUtils.jl with the following function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SymbolicUtils\n\neqn = node_to_symbolic(dominating[end].tree, options)\nprintln(simplify(eqn*5 + 3))","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can also print out the full pareto frontier like so:","category":"page"},{"location":"","page":"Home","title":"Home","text":"println(\"Complexity\\tMSE\\tEquation\")\n\nfor member in dominating\n    complexity = compute_complexity(member, options)\n    loss = member.loss\n    string = string_tree(member.tree, options)\n\n    println(\"$(complexity)\\t$(loss)\\t$(string)\")\nend","category":"page"},{"location":"#Code-structure","page":"Home","title":"Code structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SymbolicRegression.jl is organized roughly as follows. Rounded rectangles indicate objects, and rectangles indicate functions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(if you can't see this diagram being rendered, try pasting it into mermaid-js.github.io/mermaid-live-editor)","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"mermaid\">\n\nflowchart TB\n    op([Options])\n    d([Dataset])\n    op --> ES\n    d --> ES\n    subgraph ES[equation_search]\n        direction TB\n        IP[sr_spawner]\n        IP --> p1\n        IP --> p2\n        subgraph p1[Thread 1]\n            direction LR\n            pop1([Population])\n            pop1 --> src[s_r_cycle]\n            src --> opt[optimize_and_simplify_population]\n            opt --> pop1\n        end\n        subgraph p2[Thread 2]\n            direction LR\n            pop2([Population])\n            pop2 --> src2[s_r_cycle]\n            src2 --> opt2[optimize_and_simplify_population]\n            opt2 --> pop2\n        end\n        pop1 --> hof\n        pop2 --> hof\n        hof([HallOfFame])\n        hof --> migration\n        pop1 <-.-> migration\n        pop2 <-.-> migration\n        migration[migrate!]\n    end\n    ES --> output([HallOfFame])\n\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"The HallOfFame objects store the expressions with the lowest loss seen at each complexity.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The dependency structure of the code itself is as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"mermaid\">\n\nstateDiagram-v2\n    AdaptiveParsimony --> Mutate\n    AdaptiveParsimony --> Population\n    AdaptiveParsimony --> RegularizedEvolution\n    AdaptiveParsimony --> SingleIteration\n    AdaptiveParsimony --> SymbolicRegression\n    CheckConstraints --> Mutate\n    CheckConstraints --> SymbolicRegression\n    Complexity --> CheckConstraints\n    Complexity --> HallOfFame\n    Complexity --> LossFunctions\n    Complexity --> Mutate\n    Complexity --> Population\n    Complexity --> SearchUtils\n    Complexity --> SingleIteration\n    Complexity --> SymbolicRegression\n    ConstantOptimization --> Mutate\n    ConstantOptimization --> SingleIteration\n    Core --> AdaptiveParsimony\n    Core --> CheckConstraints\n    Core --> Complexity\n    Core --> ConstantOptimization\n    Core --> HallOfFame\n    Core --> InterfaceDynamicExpressions\n    Core --> LossFunctions\n    Core --> Migration\n    Core --> Mutate\n    Core --> MutationFunctions\n    Core --> PopMember\n    Core --> Population\n    Core --> Recorder\n    Core --> RegularizedEvolution\n    Core --> SearchUtils\n    Core --> SingleIteration\n    Core --> SymbolicRegression\n    Dataset --> Core\n    HallOfFame --> SearchUtils\n    HallOfFame --> SingleIteration\n    HallOfFame --> SymbolicRegression\n    InterfaceDynamicExpressions --> LossFunctions\n    InterfaceDynamicExpressions --> SymbolicRegression\n    LossFunctions --> ConstantOptimization\n    LossFunctions --> HallOfFame\n    LossFunctions --> Mutate\n    LossFunctions --> PopMember\n    LossFunctions --> Population\n    LossFunctions --> SymbolicRegression\n    Migration --> SymbolicRegression\n    Mutate --> RegularizedEvolution\n    MutationFunctions --> Mutate\n    MutationFunctions --> Population\n    MutationFunctions --> SymbolicRegression\n    Operators --> Core\n    Operators --> Options\n    Options --> Core\n    OptionsStruct --> Core\n    OptionsStruct --> Options\n    PopMember --> ConstantOptimization\n    PopMember --> HallOfFame\n    PopMember --> Migration\n    PopMember --> Mutate\n    PopMember --> Population\n    PopMember --> RegularizedEvolution\n    PopMember --> SingleIteration\n    PopMember --> SymbolicRegression\n    Population --> Migration\n    Population --> RegularizedEvolution\n    Population --> SearchUtils\n    Population --> SingleIteration\n    Population --> SymbolicRegression\n    ProgramConstants --> Core\n    ProgramConstants --> Dataset\n    ProgressBars --> SearchUtils\n    ProgressBars --> SymbolicRegression\n    Recorder --> Mutate\n    Recorder --> RegularizedEvolution\n    Recorder --> SingleIteration\n    Recorder --> SymbolicRegression\n    RegularizedEvolution --> SingleIteration\n    SearchUtils --> SymbolicRegression\n    SingleIteration --> SymbolicRegression\n    Utils --> CheckConstraints\n    Utils --> ConstantOptimization\n    Utils --> Options\n    Utils --> PopMember\n    Utils --> SingleIteration\n    Utils --> SymbolicRegression\n\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"Bash command to generate dependency structure from src directory (requires vim-stream):","category":"page"},{"location":"","page":"Home","title":"Home","text":"echo 'stateDiagram-v2'\nIFS=$'\\n'\nfor f in *.jl; do\n    for line in $(cat $f | grep -e 'import \\.\\.' -e 'import \\.'); do\n        echo $(echo $line | vims -s 'dwf:d$' -t '%s/^\\.*//g' '%s/Module//g') $(basename \"$f\" .jl);\n    done;\ndone | vims -l 'f a--> ' | sort","category":"page"},{"location":"#Search-options","page":"Home","title":"Search options","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See https://astroautomata.com/SymbolicRegression.jl/stable/api/#Options","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"examples.md\", \"api.md\", \"types.md\", \"losses.md\"]","category":"page"},{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/#Equations","page":"Types","title":"Equations","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Equations are specified as binary trees with the Node type, defined as follows:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"Node{T<:DATA_TYPE}","category":"page"},{"location":"types/#DynamicExpressions.EquationModule.Node","page":"Types","title":"DynamicExpressions.EquationModule.Node","text":"Node{T}\n\nNode defines a symbolic expression stored in a binary tree. A single Node instance is one \"node\" of this tree, and has references to its children. By tracing through the children nodes, you can evaluate or print a given expression.\n\nFields\n\ndegree::Int: Degree of the node. 0 for constants, 1 for   unary operators, 2 for binary operators.\nconstant::Bool: Whether the node is a constant.\nval::T: Value of the node. If degree==0, and constant==true,   this is the value of the constant. It has a type specified by the   overall type of the Node (e.g., Float64).\nfeature::Int (optional): Index of the feature to use in the   case of a feature node. Only used if degree==0 and constant==false.    Only defined if degree == 0 && constant == false.\nop::Int: If degree==1, this is the index of the operator   in operators.unaops. If degree==2, this is the index of the   operator in operators.binops. In other words, this is an enum   of the operators, and is dependent on the specific OperatorEnum   object. Only defined if degree >= 1\nl::Node{T}: Left child of the node. Only defined if degree >= 1.   Same type as the parent node.\nr::Node{T}: Right child of the node. Only defined if degree == 2.   Same type as the parent node. This is to be passed as the right   argument to the binary operator.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Types","title":"Types","text":"There are a variety of constructors for Node objects, including:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"Node(; val::DATA_TYPE=nothing, feature::Integer=nothing)\nNode(op::Int, l::Node)\nNode(op::Int, l::Node, r::Node)\nNode(var_string::String)","category":"page"},{"location":"types/#DynamicExpressions.EquationModule.Node-Tuple{}","page":"Types","title":"DynamicExpressions.EquationModule.Node","text":"Node([::Type{T}]; val=nothing, feature::Int=nothing) where {T}\n\nCreate a leaf node: either a constant, or a variable.\n\nArguments:\n\n::Type{T}, optionally specify the type of the   node, if not already given by the type of   val.\nval, if you are specifying a constant, pass   the value of the constant here.\nfeature::Integer, if you are specifying a variable,   pass the index of the variable here.\n\n\n\n\n\n","category":"method"},{"location":"types/#DynamicExpressions.EquationModule.Node-Tuple{Int64, Node}","page":"Types","title":"DynamicExpressions.EquationModule.Node","text":"Node(op::Int, l::Node)\n\nApply unary operator op (enumerating over the order given) to Node l\n\n\n\n\n\n","category":"method"},{"location":"types/#DynamicExpressions.EquationModule.Node-Tuple{Int64, Node, Node}","page":"Types","title":"DynamicExpressions.EquationModule.Node","text":"Node(op::Int, l::Node, r::Node)\n\nApply binary operator op (enumerating over the order given) to Nodes l and r\n\n\n\n\n\n","category":"method"},{"location":"types/#DynamicExpressions.EquationModule.Node-Tuple{String}","page":"Types","title":"DynamicExpressions.EquationModule.Node","text":"Node(var_string::String)\n\nCreate a variable node, using the format \"x1\" to mean feature 1\n\n\n\n\n\n","category":"method"},{"location":"types/","page":"Types","title":"Types","text":"When you create an Options object, the operators passed are also re-defined for Node types. This allows you use, e.g., t=Node(; feature=1) * 3f0 to create a tree, so long as * was specified as a binary operator. This works automatically for operators defined in Base, although you can also get this to work for user-defined operators by using @extend_operators:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"@extend_operators options","category":"page"},{"location":"types/#SymbolicRegression.InterfaceDynamicExpressionsModule.@extend_operators-Tuple{Any}","page":"Types","title":"SymbolicRegression.InterfaceDynamicExpressionsModule.@extend_operators","text":"@extend_operators options\n\nExtends all operators defined in this options object to work on the Node type. While by default this is already done for operators defined in Base when you create an options and pass define_helper_functions=true, this does not apply to the user-defined operators. Thus, to do so, you must apply this macro to the operator enum in the same module you have the operators defined.\n\n\n\n\n\n","category":"macro"},{"location":"types/","page":"Types","title":"Types","text":"When using these node constructors, types will automatically be promoted. You can convert the type of a node using convert:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"convert(::Type{Node{T1}}, tree::Node{T2}) where {T1, T2}","category":"page"},{"location":"types/#Base.convert-Union{Tuple{T2}, Tuple{T1}, Tuple{Type{Node{T1}}, Node{T2}}} where {T1, T2}","page":"Types","title":"Base.convert","text":"convert(::Type{Node{T1}}, n::Node{T2}) where {T1,T2}\n\nConvert a Node{T2} to a Node{T1}. This will recursively convert all children nodes to Node{T1}, using convert(T1, tree.val) at constant nodes.\n\nArguments\n\n::Type{Node{T1}}: Type to convert to.\ntree::Node{T2}: Node to convert.\n\n\n\n\n\n","category":"method"},{"location":"types/","page":"Types","title":"Types","text":"You can set a tree (in-place) with set_node!:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"set_node!(tree::Node{T}, new_tree::Node{T}) where {T}","category":"page"},{"location":"types/#DynamicExpressions.EquationModule.set_node!-Union{Tuple{T}, Tuple{Node{T}, Node{T}}} where T","page":"Types","title":"DynamicExpressions.EquationModule.set_node!","text":"set_node!(tree::Node{T}, new_tree::Node{T}) where {T}\n\nSet every field of tree equal to the corresponding field of new_tree.\n\n\n\n\n\n","category":"method"},{"location":"types/","page":"Types","title":"Types","text":"You can create a copy of a node with copy_node:","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"copy_node(tree::Node)","category":"page"},{"location":"types/#DynamicExpressions.EquationModule.copy_node-Tuple{Node}","page":"Types","title":"DynamicExpressions.EquationModule.copy_node","text":"copy_node(tree::Node; preserve_sharing::Bool=false)\n\nCopy a node, recursively copying all children nodes. This is more efficient than the built-in copy. With preserve_sharing=true, this will also preserve linkage between a node and multiple parents, whereas without, this would create duplicate child node copies.\n\nid_map is a map from objectid(tree) to copy(tree). We check against the map before making a new copy; otherwise we can simply reference the existing copy. Thanks to Ted Hopp.\n\nNote that this will not preserve loops in graphs.\n\n\n\n\n\n","category":"method"},{"location":"types/#Population","page":"Types","title":"Population","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Groups of equations are given as a population, which is an array of trees tagged with score, loss, and birthdate–-these values are given in the PopMember.","category":"page"},{"location":"types/","page":"Types","title":"Types","text":"Population(pop::Array{PopMember{T,L}, 1}) where {T<:DATA_TYPE,L<:LOSS_TYPE}\nPopulation(dataset::Dataset{T,L};\n           npop::Int, nlength::Int=3,\n           options::Options,\n           nfeatures::Int) where {T<:DATA_TYPE,L<:LOSS_TYPE}\nPopulation(X::AbstractMatrix{T}, y::AbstractVector{T};\n           npop::Int, nlength::Int=3,\n           options::Options,\n           nfeatures::Int,\n           loss_type::Type=Nothing) where {T<:DATA_TYPE}","category":"page"},{"location":"types/#SymbolicRegression.PopulationModule.Population-Union{Tuple{Array{PopMember{T, L}, 1}}, Tuple{L}, Tuple{T}} where {T<:Number, L<:Real}","page":"Types","title":"SymbolicRegression.PopulationModule.Population","text":"Population(pop::Array{PopMember{T,L}, 1})\n\nCreate population from list of PopMembers.\n\n\n\n\n\n","category":"method"},{"location":"types/#SymbolicRegression.PopulationModule.Population-Union{Tuple{Dataset{T, L, AX, AY, AW} where {AX<:AbstractMatrix{T}, AY<:Union{Nothing, AbstractVector{T}}, AW<:Union{Nothing, AbstractVector{T}}}}, Tuple{L}, Tuple{T}} where {T<:Number, L<:Real}","page":"Types","title":"SymbolicRegression.PopulationModule.Population","text":"Population(dataset::Dataset{T,L};\n           npop::Int, nlength::Int=3, options::Options,\n           nfeatures::Int)\n\nCreate random population and score them on the dataset.\n\n\n\n\n\n","category":"method"},{"location":"types/#SymbolicRegression.PopulationModule.Population-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}}} where T<:Number","page":"Types","title":"SymbolicRegression.PopulationModule.Population","text":"Population(X::AbstractMatrix{T}, y::AbstractVector{T};\n           npop::Int, nlength::Int=3,\n           options::Options, nfeatures::Int,\n           loss_type::Type=Nothing)\n\nCreate random population and score them on the dataset.\n\n\n\n\n\n","category":"method"},{"location":"types/#Population-members","page":"Types","title":"Population members","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"PopMember(t::Node{T}, score::L, loss::L) where {T<:DATA_TYPE,L<:LOSS_TYPE}\nPopMember(dataset::Dataset{T,L}, t::Node{T}, options::Options) where {T<:DATA_TYPE,L<:LOSS_TYPE}","category":"page"},{"location":"types/#SymbolicRegression.PopMemberModule.PopMember-Union{Tuple{L}, Tuple{T}, Tuple{Dataset{T, L, AX, AY, AW} where {AX<:AbstractMatrix{T}, AY<:Union{Nothing, AbstractVector{T}}, AW<:Union{Nothing, AbstractVector{T}}}, Node{T}, Options}} where {T<:Number, L<:Real}","page":"Types","title":"SymbolicRegression.PopMemberModule.PopMember","text":"PopMember(dataset::Dataset{T,L},\n          t::Node{T}, options::Options)\n\nCreate a population member with a birth date at the current time. Automatically compute the score for this tree.\n\nArguments\n\ndataset::Dataset{T,L}: The dataset to evaluate the tree on.\nt::Node{T}: The tree for the population member.\noptions::Options: What options to use.\n\n\n\n\n\n","category":"method"},{"location":"types/#Hall-of-Fame","page":"Types","title":"Hall of Fame","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"HallOfFame(options::Options, ::Type{T}, ::Type{L}) where {T<:DATA_TYPE,L<:LOSS_TYPE}","category":"page"},{"location":"types/#SymbolicRegression.HallOfFameModule.HallOfFame-Union{Tuple{L}, Tuple{T}, Tuple{Options, Type{T}, Type{L}}} where {T<:Number, L<:Real}","page":"Types","title":"SymbolicRegression.HallOfFameModule.HallOfFame","text":"HallOfFame(options::Options, ::Type{T}, ::Type{L}) where {T<:DATA_TYPE,L<:LOSS_TYPE}\n\nCreate empty HallOfFame. The HallOfFame stores a list of PopMember objects in .members, which is enumerated by size (i.e., .members[1] is the constant solution). .exists is used to determine whether the particular member has been instantiated or not.\n\nArguments:\n\noptions: Options containing specification about deterministic.\nT: Type of Nodes to use in the population. e.g., Float64.\nL: Type of loss to use in the population. e.g., Float64.\n\n\n\n\n\n","category":"method"},{"location":"types/#Dataset","page":"Types","title":"Dataset","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"Dataset{T<:DATA_TYPE,L<:LOSS_TYPE}\nDataset(X::AbstractMatrix{T},\n        y::AbstractVector{T};\n        weights::Union{AbstractVector{T}, Nothing}=nothing,\n        varMap::Union{Array{String, 1}, Nothing}=nothing,\n        loss_type::Type=Nothing,\n       ) where {T<:DATA_TYPE}\nupdate_baseline_loss!(dataset::Dataset{T,L}, options::Options) where {T<:DATA_TYPE,L<:LOSS_TYPE}","category":"page"},{"location":"types/#SymbolicRegression.CoreModule.DatasetModule.Dataset","page":"Types","title":"SymbolicRegression.CoreModule.DatasetModule.Dataset","text":"Dataset{T<:DATA_TYPE,L<:LOSS_TYPE}\n\nFields\n\nX::AbstractMatrix{T}: The input features, with shape (nfeatures, n).\ny::AbstractVector{T}: The desired output values, with shape (n,).\nn::Int: The number of samples.\nnfeatures::Int: The number of features.\nweighted::Bool: Whether the dataset is non-uniformly weighted.\nweights::Union{AbstractVector{T},Nothing}: If the dataset is weighted,   these specify the per-sample weight (with shape (n,)).\nextra::NamedTuple: Extra information to pass to a custom evaluation   function. Since this is an arbitrary named tuple, you could pass   any sort of dataset you wish to here.\navg_y: The average value of y (weighted, if weights are passed).\nuse_baseline: Whether to use a baseline loss. This will be set to false   if the baseline loss is calculated to be Inf.\nbaseline_loss: The loss of a constant function which predicts the average   value of y. This is loss-dependent and should be updated with   update_baseline_loss!.\nvariable_names::Array{String,1}: The names of the features,   with shape (nfeatures,).\n\n\n\n\n\n","category":"type"},{"location":"types/#SymbolicRegression.CoreModule.DatasetModule.Dataset-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}}} where T<:Number","page":"Types","title":"SymbolicRegression.CoreModule.DatasetModule.Dataset","text":"Dataset(X::AbstractMatrix{T}, y::Union{AbstractVector{T},Nothing}=nothing;\n        weights::Union{AbstractVector{T}, Nothing}=nothing,\n        variable_names::Union{Array{String, 1}, Nothing}=nothing,\n        extra::NamedTuple=NamedTuple(),\n        loss_type::Type=Nothing)\n\nConstruct a dataset to pass between internal functions.\n\n\n\n\n\n","category":"method"},{"location":"types/#SymbolicRegression.LossFunctionsModule.update_baseline_loss!-Union{Tuple{L}, Tuple{T}, Tuple{Dataset{T, L, AX, AY, AW} where {AX<:AbstractMatrix{T}, AY<:Union{Nothing, AbstractVector{T}}, AW<:Union{Nothing, AbstractVector{T}}}, Options}} where {T<:Number, L<:Real}","page":"Types","title":"SymbolicRegression.LossFunctionsModule.update_baseline_loss!","text":"update_baseline_loss!(dataset::Dataset{T,L}, options::Options) where {T<:DATA_TYPE,L<:LOSS_TYPE}\n\nUpdate the baseline loss of the dataset using the loss function specified in options.\n\n\n\n\n\n","category":"method"}]
}
